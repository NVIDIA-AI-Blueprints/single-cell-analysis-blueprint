{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e4dac4",
   "metadata": {},
   "source": [
    "# **Out-of-Core Single-Cell Analysis with RAPIDS-SingleCell & Dask**  \n",
    "**Author:** [Severin Dicks](https://github.com/Intron7), [T.J. Chen](https://github.com/tjchennv)\n",
    "\n",
    "**Copyright** [scverse](https://scverse.org)\n",
    "\n",
    "In this notebook, we demonstrate the **out-of-core computation** capabilities of **rapids-singlecell** using **Dask**.  \n",
    "This approach allows us to analyze larger scale datasets, such as 1.3 million to **11 million cells** efficiently, even on relatively small hardware.  \n",
    "The user also has the option to switch to 1.3 million cells (more suitable for a smaller instance).\n",
    "\n",
    "By leveraging **Dask**, we can:  \n",
    "- **Process large-scale single-cell datasets** without exceeding memory limits.  \n",
    "- **Enable chunk-based execution**, loading only the necessary data into memory at any time.  \n",
    "\n",
    "The notebook covers the full analysis pipeline:\n",
    "1. **Preprocessing** — QC, filtering, normalization, HVG selection, scaling, and PCA\n",
    "2. **Neighborhood Graph** — Using the `all_neighbors` algorithm for efficient approximate nearest neighbor search at scale\n",
    "3. **Clustering & Visualization** — Leiden clustering and UMAP embedding\n",
    "4. **Differential Expression** — Identifying marker genes per cluster with the GPU-accelerated `wilcoxon_binned` method\n",
    "\n",
    "This method makes **large-scale single-cell analysis feasible** on standard hardware setups,  \n",
    "removing barriers to working with massive datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37588552-b9d5-4113-9f53-ecc63d3da815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import time\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3a575",
   "metadata": {},
   "source": [
    "**RMM (RAPIDS Memory Manager)** controls how GPU memory is allocated during the analysis.\n",
    "\n",
    "By default, memory requests go to the GPU driver, which can become a bottleneck when processing millions of cells. RMM **pre-allocates a large memory pool upfront** and serves requests from it internally, keeping allocations fast throughout the pipeline.\n",
    "\n",
    "The `rmm_cupy_allocator` ensures that both RAPIDS and CuPy draw from the **same pool**, rather than maintaining separate ones that can fragment and cause out-of-memory errors mid-analysis.\n",
    "\n",
    "> **In practice:** The pool sizes you set in the Dask cluster configuration above are what back this allocator, so these two setup steps work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f159c97-e435-40c5-ac7b-aa37a6812ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rmm #RAPIDS Memory Manager (controls how GPU memory is allocated and managed)\n",
    "import cupy as cp\n",
    "import rapids_singlecell as rsc\n",
    "from rmm.allocators.cupy import rmm_cupy_allocator #ensures that both RAPIDS and CuPy draw from the same memory pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k08skcdye5r",
   "source": "## **Auto-Selecting Dataset Based on Available GPU Memory**\n\nThis cell detects your GPU configuration and automatically selects the appropriate dataset:\n- **< 96 GB** total GPU memory → 1.3M cell dataset (single GPU setups)\n- **≥ 96 GB** total GPU memory → 11M cell dataset (multi-GPU setups)\n\nIt also sets `preprocessing_gpus` to use all available GPUs automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7zqvxk6noxr",
   "source": "import pynvml\n\n# Initialize NVML to query GPU hardware information\npynvml.nvmlInit()\nn_gpus = pynvml.nvmlDeviceGetCount()\n\n# Collect name and memory (in GB) for each available GPU\ngpu_info = [\n    (pynvml.nvmlDeviceGetName(pynvml.nvmlDeviceGetHandleByIndex(i)),\n     pynvml.nvmlDeviceGetMemoryInfo(pynvml.nvmlDeviceGetHandleByIndex(i)).total / 1024**3)\n    for i in range(n_gpus)\n]\n\n# Use GPU 0 memory as the reference — all GPUs on a given instance are typically identical,\n# and this ensures dataset selection reflects true per-GPU capacity rather than aggregate totals\ngpu0_memory_gb = gpu_info[0][1]\n\npreprocessing_gpus = \"all\"  # Uses all available GPUs. To target specific devices, replace with a comma-separated index string (e.g. \"0,1\") — indices match the GPU numbers printed below\n\n# Select dataset based on per-GPU memory\nif gpu0_memory_gb < 24:\n    url, output, final, dataset = None, None, None, None\n    print(f\"WARNING: GPU 0 has only {gpu0_memory_gb:.1f} GB of memory.\")\n    print(\"This notebook requires at least 24 GB of GPU memory to run.\")\n    print(\"Please use a larger instance (e.g. L40S 48 GB or higher) for best performance.\")\nelif gpu0_memory_gb >= 96:\n    url    = \"https://datasets.cellxgene.cziscience.com/3817734b-0f82-433b-8c38-55b214200fff.h5ad\"\n    output = \"./h5/cell_atlas.h5ad\"\n    final  = \"./zarr/cell_atlas.zarr\"\n    dataset = \"11M Cell Atlas\"\nelse:\n    url    = \"https://exampledata.scverse.org/rapids-singlecell/1M_brain_cells_10X.sparse.h5ad\"\n    output = \"./h5/nvidia_1.3M.h5ad\"\n    final  = \"./zarr/nvidia_1.3M.zarr\"\n    dataset = \"1.3M Brain Cells\"\n\n# Print a summary of the detected hardware and selected configuration\ngpu_rows = \"\\n\".join(f\"  GPU {i}: {name} ({mem:.1f} GB)\" for i, (name, mem) in enumerate(gpu_info))\nprint(f\"GPU Configuration\\n{'-'*40}\\n{gpu_rows}\\n{'-'*40}\\nDataset selected: {dataset}\\nGPUs in use:      {preprocessing_gpus}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a81da4d7",
   "metadata": {},
   "source": [
    "## **Initializing a Dask Cluster for Out-of-Core Computation**  \n",
    "\n",
    "To enable **out-of-core computation** and parallel processing,  \n",
    "we initialize a **Dask CUDA cluster**, which distributes computations across available GPU resources.  \n",
    "\n",
    "We create a **local GPU cluster** with the following configuration:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79faf22b-629e-43d6-a762-02dd1fa2e65c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "%%time\ncluster = LocalCUDACluster(CUDA_VISIBLE_DEVICES=preprocessing_gpus,\n                           threads_per_worker=10,\n                           protocol=\"ucx\",\n                           rmm_pool_size= \"10GB\",                        # Pre-allocate GPU memory upfront to avoid repeated OS requests\n                           rmm_maximum_pool_size = \"40GB\",                # Hard cap on pool growth — keep this <= your GPU's VRAM\n                           rmm_allocator_external_lib_list= \"cupy\",       # Route CuPy allocations through RMM for unified memory management\n                          )\n\nclient = Client(cluster)\n\nclient"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ac3268-f54b-40c6-9f56-6f0d942c6753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rapids_singlecell as rsc\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"./scripts\")\n",
    "from gpu_plotting import plot_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94f7c9",
   "metadata": {},
   "source": "## **Loading Large Datasets into AnnData with Dask**  \n\nTo efficiently handle large-scale single-cell datasets, we load data directly from an **HDF5 (`h5`) or Zarr file**  \ninto an **AnnData object** using **Dask arrays**. This enables **lazy loading**, allowing data to be processed in chunks  \nwithout exceeding memory limits.  \n\nWe achieve this using **`read_elem_lazy`**, which loads the expression matrix (`X`) as a **Dask array**\n\nLet's first download the data. The dataset has been automatically selected based on your GPU memory in the configuration cell above.\n- For the 1.3M cells dataset, `nvidia_1.3M`, this may take about 5 minutes, as it is 5.3 gigabytes to download and convert.\n- For the 11M cells dataset, `cell_atlas`, downloading can get considerable time, as it is over 43GB of data to download and convert.\n\nIf you already have the data ready, it will skip the download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1308986-5fc0-47ec-97e3-565992d68229",
   "metadata": {},
   "outputs": [],
   "source": "import wget\nimport os\nfrom anndata.experimental import read_elem_lazy\nimport h5py\n\ndata_dir = \"./h5\"\n\nif not os.path.exists(final):\n    if not os.path.exists(data_dir):\n        print('creating data directory')\n        os.system('mkdir ./h5')\n    else:\n        print(f'{data_dir} directory found')\n\n    if not os.path.isfile(output):\n        print(f'Downloading cell data into {output}...')\n        wget.download(url, output)\n    else:\n        print(f'{output} dataset found')\n\n    print(f'Converting {output} into {final}...')\n    SPARSE_CHUNK_SIZE = 20_000\n\n    f = h5py.File(output)\n    X = f[\"X\"]\n    shape = X.attrs[\"shape\"]\n    adata = ad.AnnData(\n        X = read_elem_lazy(X, (SPARSE_CHUNK_SIZE, shape[1])),\n        obs = ad.io.read_elem(f[\"obs\"]),\n        var = ad.io.read_elem(f[\"var\"]))\n    f.close()\n\n    adata.write_zarr(final)\n    print(f'{final} is ready for use!')\nelse:\n    print(f'{final} zarr dataset directory found')"
  },
  {
   "cell_type": "markdown",
   "id": "872bd56a-1766-46ea-a466-b79a136af90f",
   "metadata": {},
   "source": [
    "Now, let's read in the zarr data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc9a948-80ea-457f-8ded-151505a23418",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "SPARSE_CHUNK_SIZE = 20_000\n",
    "\n",
    "f = zarr.open(final)\n",
    "X = f[\"X\"]\n",
    "shape = X.attrs[\"shape\"]\n",
    "adata = ad.AnnData(\n",
    "    X = read_elem_lazy(X, (SPARSE_CHUNK_SIZE, shape[1])),\n",
    "    obs = ad.io.read_elem(f[\"obs\"]),\n",
    "    var = ad.io.read_elem(f[\"var\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a6124e-c519-4d1d-b1d5-6ae8009f3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var_names = adata.var.feature_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8246527-1102-4747-b328-939800567142",
   "metadata": {},
   "source": [
    "## **Transferring AnnData to GPU for Accelerated Computation**  \n",
    "\n",
    "Once the dataset is loaded as a **Dask-backed AnnData object**,  \n",
    "we transfer it to the **GPU** to leverage **RAPIDS-SingleCell’s** accelerated processing.  \n",
    "\n",
    "We use **`rsc.get.anndata_to_GPU()`**, which efficiently moves the AnnData object to GPU memory:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "114352ef-3262-4103-a1a5-f9b33b712656",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsc.get.anndata_to_GPU(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2762631c-1b7d-49da-8bd4-3c9d5a0a0ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (11441407, 45854) </td>\n",
       "                        <td> (20000, 45854) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 573 chunks in 2 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 cupyx.scipy.sparse._csr.csr_matrix </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"56\" x2=\"25\" y2=\"56\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"75\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"101\" x2=\"25\" y2=\"101\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.41261651458248,0.0 25.41261651458248,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.70630825729124\" y=\"140.0\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >45854</text>\n",
       "  <text x=\"45.41261651458248\" y=\"60.0\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.41261651458248,60.0)\">11441407</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<X_to_GPU, shape=(11441407, 45854), dtype=float32, chunksize=(20000, 45854), chunktype=cupyx.csr_matrix>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03b477-7b57-4ba9-895f-78c0a1ead5ff",
   "metadata": {},
   "source": [
    "## **Quality Control (QC) Metrics Calculation (Requires Synchronization)**  \n",
    "\n",
    "Before proceeding with further analysis, we compute **quality control (QC) metrics**  \n",
    "to assess dataset quality and filter out low-quality cells or genes.  \n",
    "\n",
    "We use **`rsc.pp.calculate_qc_metrics()`** to calculate key QC metrics\n",
    "\n",
    "Although we are working with Dask-backed AnnData, this operation requires a synchronization step.\n",
    "This means that Dask computations must be evaluated immediately,\n",
    "so the process is not completely lazy like other out-of-core operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f4f2b47-d363-40c1-b168-cff95b084358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.7 s, sys: 1.45 s, total: 44.1 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.pp.calculate_qc_metrics(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daf379-e37e-46ed-91d8-53dbb6bc0563",
   "metadata": {},
   "source": [
    "## **Filtering Cells and Genes Without Additional Computation**  \n",
    "\n",
    "Instead of using **`sc.pp.filter_cells`** and **`sc.pp.filter_genes`**,  \n",
    "we apply filtering directly using boolean indexing to **avoid extra computation**.\n",
    "\n",
    "**Why Use Direct Indexing Instead of Built-in Functions?**\n",
    "* More Efficient with Dask → Avoids triggering additional computations.\n",
    "* Preserves Lazy Execution → Filtering is applied without forcing full dataset evaluation.\n",
    "* Copy is Essential → Using `.copy()` prevents views, which may not work reliably with Dask-backed AnnData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f90ad3-52ac-47da-8840-ccf9db43c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[(adata.obs[\"n_genes_by_counts\"]<=10000) \n",
    "            & (adata.obs[\"n_genes_by_counts\"]>=200)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299580e9-44a9-41c5-bfe2-570ddd61ce7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (11441244, 45854) </td>\n",
       "                        <td> (20120, 45854) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 573 chunks in 3 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 cupyx.scipy.sparse._csr.csr_matrix </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"6\" x2=\"25\" y2=\"6\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"25\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"18\" x2=\"25\" y2=\"18\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" />\n",
       "  <line x1=\"0\" y1=\"31\" x2=\"25\" y2=\"31\" />\n",
       "  <line x1=\"0\" y1=\"37\" x2=\"25\" y2=\"37\" />\n",
       "  <line x1=\"0\" y1=\"44\" x2=\"25\" y2=\"44\" />\n",
       "  <line x1=\"0\" y1=\"50\" x2=\"25\" y2=\"50\" />\n",
       "  <line x1=\"0\" y1=\"56\" x2=\"25\" y2=\"56\" />\n",
       "  <line x1=\"0\" y1=\"63\" x2=\"25\" y2=\"63\" />\n",
       "  <line x1=\"0\" y1=\"69\" x2=\"25\" y2=\"69\" />\n",
       "  <line x1=\"0\" y1=\"75\" x2=\"25\" y2=\"75\" />\n",
       "  <line x1=\"0\" y1=\"82\" x2=\"25\" y2=\"82\" />\n",
       "  <line x1=\"0\" y1=\"88\" x2=\"25\" y2=\"88\" />\n",
       "  <line x1=\"0\" y1=\"94\" x2=\"25\" y2=\"94\" />\n",
       "  <line x1=\"0\" y1=\"100\" x2=\"25\" y2=\"100\" />\n",
       "  <line x1=\"0\" y1=\"107\" x2=\"25\" y2=\"107\" />\n",
       "  <line x1=\"0\" y1=\"113\" x2=\"25\" y2=\"113\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.41261651458248,0.0 25.41261651458248,120.0 0.0,120.0\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.70630825729124\" y=\"140.0\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >45854</text>\n",
       "  <text x=\"45.41261651458248\" y=\"60.0\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.41261651458248,60.0)\">11441244</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<getitem, shape=(11441244, 45854), dtype=float32, chunksize=(20120, 45854), chunktype=cupyx.csr_matrix>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15ca8b-672b-40e2-bc64-4df3c977da83",
   "metadata": {},
   "source": [
    "## **Log Normalization (Fully Lazy Execution)**  \n",
    "\n",
    "Next, we apply **log normalization** to scale gene expression values.  \n",
    "This step ensures that differences in sequencing depth across cells do not dominate downstream analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdbe6f8f-9086-474b-9583-d85b9190593f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.56 ms, sys: 974 μs, total: 6.53 ms\n",
      "Wall time: 5.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.pp.normalize_total(adata,target_sum = 10000)\n",
    "rsc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03588f87-4603-46f6-aa9a-1f16f05f1ac6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Selecting Highly Variable Genes (Requires Synchronization)**  \n",
    "\n",
    "To focus on the most informative features, we identify **highly variable genes (HVGs)**  \n",
    "using the **Cell Ranger** method and subset the dataset accordingly.  \n",
    "\n",
    "**Important Considerations:**\n",
    "* Requires Synchronization → Computing highly variable genes triggers evaluation,\n",
    "meaning this step is not fully lazy when using Dask.\n",
    "* Copy is Essential → Using `.copy()` prevents views, ensuring the operation works properly with Dask-backed AnnData.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb84f792-2b06-4bfa-8cca-3b4eaec042ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdicks/micromamba/envs/rapids-26.02/lib/python3.13/site-packages/distributed/client.py:3375: UserWarning: Sending large graph of size 22.25 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 1.83 s, total: 1min 2s\n",
      "Wall time: 55.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.pp.highly_variable_genes(adata,n_top_genes=5000, flavor=\"cell_ranger\")\n",
    "adata = adata[:,adata.var.highly_variable].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c116d8-05cf-4efb-a25d-726e0c0f6c3f",
   "metadata": {},
   "source": [
    "## **Principal Component Analysis (PCA) on GPU (Two-Step Synchronization Process)**  \n",
    "\n",
    "To reduce dimensionality while preserving meaningful variation,  \n",
    "we perform **Principal Component Analysis (PCA)** using **GPU acceleration**.\n",
    "\n",
    "Understanding the Two-Step Synchronization in PCA:\n",
    "1. Mandatory Synchronization for Covariance & Mean Calculation\n",
    "    * PCA requires computing the covariance matrix and mean vector,\n",
    "    which must be explicitly synchronized before proceeding.\n",
    "    * This step is handled automatically within `rsc.pp.pca()`.\n",
    "\n",
    "2. Finalizing the Transformation with `.compute()`\n",
    "    * After computing the principal components, the data remains lazy (Dask CuPy array).\n",
    "    * Calling `.compute()` on `adata.obsm[\"X_pca\"]` performs the final transformation,\n",
    "      projecting the data onto the computed PCs and materializing the result as a fully computed CuPy array.\n",
    "\n",
    "**Why This Matters?**\n",
    "* The first synchronization (**covariance & mean**) ensures the PCA model is ready.\n",
    "* The second synchronization (`compute()`) ensures that the transformed data is fully realized\n",
    "for downstream analyses like clustering and UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b525a19c-6ed6-4e2c-a249-c4fc536862bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.7 s, sys: 2.23 s, total: 57.9 s\n",
      "Wall time: 57.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.pp.pca(adata, n_comps = 100,mask_var=None)\n",
    "adata.obsm[\"X_pca\"]=adata.obsm[\"X_pca\"].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d18278-267d-4fad-80cb-afa88a45543f",
   "metadata": {},
   "source": [
    "## **Computing the Neighborhood Graph with `all_neighbors`**\n",
    "\n",
    "Next, we compute the neighborhood graph using the **`all_neighbors`** algorithm.\n",
    "Unlike brute-force search, which computes distances to every data point,\n",
    "`all_neighbors` uses an approximate nearest neighbor approach optimized for large-scale datasets.\n",
    "\n",
    "This algorithm is well-suited for out-of-core workflows where datasets can contain millions of cells,\n",
    "as it provides a good balance between accuracy and computational efficiency on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fvos0s5s67r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 7s, sys: 13 s, total: 4min 20s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.pp.neighbors(adata, n_neighbors=15, n_pcs=50, algorithm=\"all_neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3aj0aza6y",
   "metadata": {},
   "source": [
    "## **UMAP Embedding**\n",
    "\n",
    "We compute a **UMAP** (Uniform Manifold Approximation and Projection) embedding\n",
    "to visualize the high-dimensional data in two dimensions.\n",
    "The RAPIDS GPU implementation of UMAP is significantly faster than CPU-based methods,\n",
    "making it practical even for datasets with millions of cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mvqs67mb38s",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 4.33 s, total: 24.6 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kknpqm8gc9",
   "metadata": {},
   "source": [
    "## **Leiden Clustering**\n",
    "\n",
    "We use the **Leiden** algorithm for graph-based clustering.\n",
    "Leiden is an improved version of the Louvain algorithm that guarantees well-connected, more stable clusters.\n",
    "The RAPIDS implementation accelerates the clustering on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22t8l94az4l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 5.06 s, total: 20.5 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rsc.tl.leiden(adata, resolution=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "wca6fjptdr",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (2098790885.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplot_umap(adata, color_key=\"leiden\", background=None))\u001b[39m\n                                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "plot_umap(adata, color_key=\"leiden\", background=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e586236",
   "metadata": {},
   "source": [
    "**Freeing GPU Memory Before Continuing**\n",
    "\n",
    "Unlike CPU RAM, **GPU memory is not automatically released** when analysis steps complete —\n",
    "data from every preceding step (the full expression matrix, PCA, neighbor graph, UMAP embeddings)\n",
    "remains loaded on the GPU until the kernel is explicitly shut down.\n",
    "\n",
    "For an 11M cell dataset, this can consume most or all of available VRAM, leaving no room for\n",
    "downstream steps in subsequent notebooks.\n",
    "\n",
    "Restarting the kernel here is the cleanest way to guarantee a full reset of GPU memory.\n",
    "\n",
    "> **Expected behavior:** You will see a \"kernel died\" or disconnection message — this is intentional.\n",
    "> If you encounter a CUDA or Out-of-Memory (OOM) error at any earlier point in the notebook,\n",
    "> restarting all kernels and starting fresh is the recommended first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These notebooks are very GPU memory intensive!\n",
    "# You can comment this out if you want to continue exploring the notebook.\n",
    "# Please consult the README for more tips and tricks.\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}